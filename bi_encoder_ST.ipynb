{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63f3236-5e5b-4e1d-aa65-1606014a88f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankur/mambaforge/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d67bd41-1512-48d7-b810-270099de0caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import losses, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7986d-cf55-4fb1-be0d-12c5703d0e18",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2fa1d7b-99b1-4d87-92a6-842c72aa6d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27383, 4), (9090, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "train_df = pd.read_csv('data/train_df.csv')\n",
    "val_df = pd.read_csv('data/val_df.csv')\n",
    "train_df.score = (train_df.score >= 0.5).astype(int)\n",
    "val_df.score = (val_df.score >= 0.5).astype(int)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1c8ca-60e2-4dc4-b123-f7afcdfafe77",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946ca068-8610-4109-b14e-661a23274e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_df, val_df, include_context=False):\n",
    "    train_samples = []\n",
    "    val_samples = []\n",
    "    \n",
    "    for i, row in train_df.iterrows():\n",
    "        texts = [row['anchor'], row['target']] \n",
    "        if include_context:\n",
    "            texts += [row['context'],]\n",
    "        inp_example = InputExample(texts=texts, label=row['score'])\n",
    "        train_samples.append(inp_example)\n",
    "\n",
    "    for i, row in val_df.iterrows():\n",
    "        texts = [row['anchor'], row['target']]\n",
    "        if include_context:\n",
    "            texts += [row['context'],]\n",
    "        inp_example = InputExample(texts=texts, label=row['score'])\n",
    "        val_samples.append(inp_example)            \n",
    "    return train_samples, val_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee29c7-d10d-4cff-9d32-b6358df49d46",
   "metadata": {},
   "source": [
    "# Bi Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982e5bdf-f8a0-454b-9ae5-9a279b081c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "bs = 32\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44778a2a-0b16-41f9-a08e-be1b37d3dc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(pool=True):\n",
    "    word_embedding_model = models.Transformer(model_name, do_lower_case=True)\n",
    "    if not pool:\n",
    "        # word_embedding_model.auto_model.config.output_hidden_states = True\n",
    "        model = SentenceTransformer(modules=[word_embedding_model,])\n",
    "        return model\n",
    "        \n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                                   pooling_mode_mean_tokens=True,\n",
    "                                   pooling_mode_cls_token=False,\n",
    "                                   pooling_mode_max_tokens=False)\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47271e11-b284-4c53-87aa-31a6cfe63bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "from sentence_transformers.util import batch_to_device\n",
    "\n",
    "class LabelAccuracyEvaluator(SentenceEvaluator):\n",
    "    def __init__(self, dataloader: DataLoader, model = None):\n",
    "        self.dataloader = dataloader\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self) -> float:\n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        self.dataloader.collate_fn = self.model.model.smart_batching_collate\n",
    "        device = self.model.model.device\n",
    "        for step, batch in enumerate(self.dataloader):\n",
    "            features, label_ids = batch\n",
    "            for idx in range(len(features)):\n",
    "                features[idx] = batch_to_device(features[idx], device)\n",
    "            label_ids = label_ids.to(device)\n",
    "            with torch.no_grad():\n",
    "                _, prediction = self.model(features, labels=None)\n",
    "\n",
    "            total += prediction.size(0)\n",
    "            correct += torch.argmax(prediction, dim=1).eq(label_ids).sum().item()\n",
    "        accuracy = correct/total\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\\n\")\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd60e9f9-92e4-4b88-a7a6-17e9bae44d5a",
   "metadata": {},
   "source": [
    "### Without Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48371669-3dd1-472a-b0ba-9936f6cd415e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preparing data\n",
    "train_samples, val_samples = prepare_data(train_df, val_df, include_context=False)\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=bs)\n",
    "val_dataloader = DataLoader(val_samples, shuffle=False, batch_size=bs)\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e79081f-effa-4094-bf17-e9d6d1a1c4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, model, emb_dim: int, num_labels: int, loss_fct=nn.BCEWithLogitsLoss()):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_labels = num_labels\n",
    "        self.classifier = nn.Linear(2*emb_dim, num_labels)\n",
    "        self.loss_fct = loss_fct\n",
    "\n",
    "    def forward(self, sentence_features, labels):\n",
    "        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        rep_a, rep_b = reps\n",
    "        features = torch.cat([rep_a, rep_b], 1)\n",
    "        output = self.classifier(features)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(output.view(-1), labels.float())\n",
    "            return loss\n",
    "        else:\n",
    "            return reps, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54fbcdb5-fd88-4327-9be3-d0d40c1e1c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "train_loss = ClassifierHead(model=model, num_labels=1, emb_dim=model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f259b010-7c80-48de-80e1-9c8b03f31526",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 11s, sys: 7min 19s, total: 29min 30s\n",
      "Wall time: 19min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "511436b0-ba1a-44d2-8b5a-b7b6482417c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5784196241218242"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator = LabelAccuracyEvaluator(val_dataloader, train_loss)\n",
    "test_evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea3b75-6846-46a6-b3c2-f1f451750212",
   "metadata": {},
   "source": [
    "### With Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f9b30c-f473-4e0d-b02d-0b457138c47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_samples, val_samples = prepare_data(train_df, val_df, include_context=True)\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=bs)\n",
    "val_dataloader = DataLoader(val_samples, shuffle=False, batch_size=bs)\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbc318-2650-4fbb-a26f-c52b29b0bd61",
   "metadata": {},
   "source": [
    "### 1. Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "725a1477-85a8-41a7-b8bb-4f162fb0e852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassifierHeadConcatContext(nn.Module):\n",
    "    def __init__(self, model, emb_dim: int, num_labels: int, loss_fct=nn.BCEWithLogitsLoss()):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_labels = num_labels\n",
    "        self.classifier = nn.Linear(3*emb_dim, num_labels)\n",
    "        self.loss_fct = loss_fct\n",
    "\n",
    "    def forward(self, sentence_features, labels):\n",
    "        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        rep_a, rep_b, rep_c = reps\n",
    "        features = torch.cat([rep_a, rep_b, rep_c], 1)\n",
    "        output = self.classifier(features)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(output.view(-1), labels.float())\n",
    "            return loss\n",
    "        else:\n",
    "            return reps, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e39d04d4-4221-4fcf-bff3-8473d6e147eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "train_loss = ClassifierHeadConcatContext(model=model, num_labels=1, emb_dim=model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d68ac601-e9a8-464a-9db6-e6765917d20f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38min 56s, sys: 12min 14s, total: 51min 11s\n",
      "Wall time: 35min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc138b13-b600-4b36-9726-d2f1308dd816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5693061451101133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator = LabelAccuracyEvaluator(val_dataloader, train_loss)\n",
    "test_evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d430a-9781-46f3-877f-0ff66b4e4f9c",
   "metadata": {},
   "source": [
    "### 2. Element-wise Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c20bedf6-9c2a-456b-8e9e-9dbc26a29016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassifierHeadAddContext(nn.Module):\n",
    "    def __init__(self, model, emb_dim: int, num_labels: int, loss_fct=nn.BCEWithLogitsLoss()):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_labels = num_labels\n",
    "        self.classifier = nn.Linear(emb_dim, num_labels)\n",
    "        self.loss_fct = loss_fct\n",
    "\n",
    "    def forward(self, sentence_features, labels):\n",
    "        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        rep_a, rep_b, rep_c = reps\n",
    "        features = (rep_a + rep_b + rep_c)/3\n",
    "        output = self.classifier(features)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(output.view(-1), labels.float())\n",
    "            return loss\n",
    "        else:\n",
    "            return reps, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc9205ed-9b00-4231-9b40-02564cf5ceb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "train_loss = ClassifierHeadAddContext(model=model, num_labels=1, emb_dim=model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89be49ac-c606-406f-b17a-f199140067a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38min 59s, sys: 12min 19s, total: 51min 19s\n",
      "Wall time: 35min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b216dde7-f39c-4999-bdb4-8abb940e2e97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5519564366994452"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator = LabelAccuracyEvaluator(val_dataloader, train_loss)\n",
    "test_evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335e38f-ef35-497e-b697-85d82d69ca55",
   "metadata": {},
   "source": [
    "### 3. Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eb8b74e-9a9b-4c5d-8aca-f5d1b3f6dda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassifierHeadAttentionContext(nn.Module):\n",
    "    def __init__(self, model, emb_dim: int, num_labels: int, loss_fct=nn.BCEWithLogitsLoss()):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_labels = num_labels\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.multihead_attn = nn.MultiheadAttention(emb_dim, 2, batch_first=True)\n",
    "        self.classifier = nn.Linear(2*emb_dim, num_labels)\n",
    "        self.loss_fct = loss_fct\n",
    "\n",
    "    def forward(self, sentence_features, labels):\n",
    "        reps = [self.model(sentence_feature)['token_embeddings'] for sentence_feature in sentence_features]\n",
    "        rep_a, rep_b, rep_c = reps\n",
    "        \n",
    "        rep_a = self.ln(rep_a)\n",
    "        rep_b = self.ln(rep_b)\n",
    "        rep_c = self.ln(rep_c)\n",
    "        rep_attn_a, _ = self.multihead_attn(rep_a, rep_c, rep_c)\n",
    "        rep_attn_b, _ = self.multihead_attn(rep_b, rep_c, rep_c)\n",
    "        rep_attn_a = self.ln(rep_attn_a)\n",
    "        rep_attn_b = self.ln(rep_attn_b)\n",
    "        \n",
    "        rep_attn_a = rep_attn_a.mean(1)\n",
    "        rep_attn_b = rep_attn_b.mean(1)\n",
    "        features = torch.concat([rep_attn_a, rep_attn_b], 1)\n",
    "\n",
    "        output = self.classifier(features)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(output.view(-1), labels.float())\n",
    "            return loss\n",
    "        else:\n",
    "            return reps, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f677c9e9-9aa3-4c52-885a-1c526150e0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = get_model(pool=False)\n",
    "train_loss = ClassifierHeadAttentionContext(model=model, num_labels=1, emb_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49f2dcfd-6ebe-4f20-b252-821f6b83610e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40min 20s, sys: 12min 41s, total: 53min 1s\n",
      "Wall time: 37min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b716c331-2de1-4594-b4ff-452aa8216c74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5271 (4791/9090)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.527062706270627"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator = LabelAccuracyEvaluator(val_dataloader, train_loss)\n",
    "test_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bd6a2-ac84-4fd6-9f83-50d73183e052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
